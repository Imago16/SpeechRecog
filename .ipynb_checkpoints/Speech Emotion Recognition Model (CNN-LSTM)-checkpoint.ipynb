{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition Model CNN/LSTM | Part 3\n",
    "\n",
    "## Overview of the Notebook\n",
    "\n",
    "This code is a notebook focused on **loading a pre-trained Speech Emotion Recognition (SER) model, performing feature extraction on an audio file, and making a prediction** based on the emotional content of the audio.\n",
    "\n",
    "### Key Sections\n",
    "\n",
    "1. **Library Imports**:\n",
    "   - Essential libraries for audio processing (`librosa`, `pydub`), data handling (`numpy`, `pickle`), and deep learning (`tensorflow`) are imported.\n",
    "   - `IPython.display` is used to play audio files directly in the notebook.\n",
    "\n",
    "\n",
    "2. **Model and Preprocessing Artifacts Loading**:\n",
    "   - Loads the pre-trained CNN/LSTM model structure and weights from JSON and HDF5 files.\n",
    "   - Loads the `StandardScaler` and `OneHotEncoder` objects saved from previous preprocessing steps. These are used to standardize and encode features for consistent input into the model.\n",
    "\n",
    "\n",
    "3. **Audio File Analysis**:\n",
    "   - The code takes an input audio file, `happiness.wav`, and loads it with `librosa`.\n",
    "   - Basic visualization is done using:\n",
    "     - A **waveform plot** to display the time-domain representation.\n",
    "     - A **spectrum plot** to analyze frequency components.\n",
    "     - A **Mel spectrogram** to examine energy across different frequencies, providing more insights into tonal quality.\n",
    "\n",
    "\n",
    "4. **Feature Extraction**:\n",
    "   - Defines several helper functions to extract features from the audio:\n",
    "     - `zcr()`: Calculates zero-crossing rate, capturing signal noisiness.\n",
    "     - `rmse()`: Calculates root-mean-square energy, giving amplitude information.\n",
    "     - `mfcc()`: Extracts Mel Frequency Cepstral Coefficients, essential for capturing human voice characteristics.\n",
    "   - Combines these features in `extract_features()`, creating a comprehensive feature vector for model input.\n",
    "\n",
    "\n",
    "5. **Prediction Function**:\n",
    "   - **Feature Preparation**:\n",
    "     - `get_predict_feat()`: Takes the file path, extracts and standardizes features, ensuring the feature vector matches the input shape expected by the model.\n",
    "   - **Prediction**:\n",
    "     - `prediction()`: Uses the pre-trained model to predict the emotional state in the input audio. It identifies the emotion with the highest confidence and displays each emotion label's confidence score, providing insights into the model's certainty.\n",
    "\n",
    "\n",
    "6. **Results**:\n",
    "   - The predicted emotion label and associated confidence scores are displayed, allowing a review of model performance for each emotion class.\n",
    "\n",
    "\n",
    "### Key Points\n",
    "- **Scalability**: Can be adapted for different audio files by adjusting `input_path`.\n",
    "- **Interpretability**: Provides confidence scores for each class, which can be helpful for analysis.\n",
    "- **Use Case**: This setup is suited for testing or deploying SER on new audio files, using pre-saved model weights and preprocessing artifacts.\n",
    "\n",
    "This notebook efficiently serves as a standalone tool for real-time emotion detection in audio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UDy0_duByOcE"
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:50:20.470244Z",
     "iopub.status.busy": "2024-11-03T10:50:20.468993Z",
     "iopub.status.idle": "2024-11-03T10:50:34.728098Z",
     "shell.execute_reply": "2024-11-03T10:50:34.726597Z",
     "shell.execute_reply.started": "2024-11-03T10:50:20.470184Z"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1712643108954,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "enH-qOdOyItj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aegis/Research/Machine Learning in HEP/mlenv/lib/python3.12/site-packages/pydub/utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n",
      "2025-04-15 16:09:02.944863: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-15 16:09:02.954773: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744704542.966388    9796 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744704542.969675    9796 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1744704542.978303    9796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744704542.978320    9796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744704542.978321    9796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1744704542.978322    9796 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-04-15 16:09:02.981535: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import librosa\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pydub as pyd\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VP1v8OeVykmT"
   },
   "source": [
    "# Import Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBojSjhCysqD"
   },
   "source": [
    "## Speech Emotion Recognition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:50:42.209078Z",
     "iopub.status.busy": "2024-11-03T10:50:42.207819Z",
     "iopub.status.idle": "2024-11-03T10:50:42.219887Z",
     "shell.execute_reply": "2024-11-03T10:50:42.218729Z",
     "shell.execute_reply.started": "2024-11-03T10:50:42.209003Z"
    },
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1712643131599,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "xJSeY-UhynN6"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/CNN_model.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m json_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/CNN_model.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m loaded_model_json \u001b[38;5;241m=\u001b[39m json_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      3\u001b[0m json_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/CNN_model.json'"
     ]
    }
   ],
   "source": [
    "json_file = open('/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/CNN_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:50:55.379229Z",
     "iopub.status.busy": "2024-11-03T10:50:55.378701Z",
     "iopub.status.idle": "2024-11-03T10:50:56.9948Z",
     "shell.execute_reply": "2024-11-03T10:50:56.993587Z",
     "shell.execute_reply.started": "2024-11-03T10:50:55.379184Z"
    },
    "executionInfo": {
     "elapsed": 2344,
     "status": "ok",
     "timestamp": 1712643133939,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "J3hAwbGXzEni"
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/best_model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:51:11.465425Z",
     "iopub.status.busy": "2024-11-03T10:51:11.46435Z",
     "iopub.status.idle": "2024-11-03T10:51:11.915676Z",
     "shell.execute_reply": "2024-11-03T10:51:11.914637Z",
     "shell.execute_reply.started": "2024-11-03T10:51:11.465377Z"
    },
    "executionInfo": {
     "elapsed": 1650,
     "status": "ok",
     "timestamp": 1712643135586,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "H0pP1E8NzXUK"
   },
   "outputs": [],
   "source": [
    "with open('/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/scaler.pickle', 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "\n",
    "with open('/kaggle/input/speech-emotion-recognition-model-cnn-lstm-part-2/encoder.pickle', 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U77FQXet1jex"
   },
   "source": [
    "# 1. Audio file Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:31.064489Z",
     "iopub.status.busy": "2024-11-03T10:56:31.064011Z",
     "iopub.status.idle": "2024-11-03T10:56:31.071139Z",
     "shell.execute_reply": "2024-11-03T10:56:31.069764Z",
     "shell.execute_reply.started": "2024-11-03T10:56:31.064447Z"
    },
    "id": "no6VjHdv8lQk"
   },
   "outputs": [],
   "source": [
    "# set input path to any audio file\n",
    "input_path = \"/kaggle/input/serm-testing/audio.wav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:31.353815Z",
     "iopub.status.busy": "2024-11-03T10:56:31.353301Z",
     "iopub.status.idle": "2024-11-03T10:56:31.532483Z",
     "shell.execute_reply": "2024-11-03T10:56:31.531245Z",
     "shell.execute_reply.started": "2024-11-03T10:56:31.353766Z"
    },
    "executionInfo": {
     "elapsed": 12176,
     "status": "ok",
     "timestamp": 1708171712429,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "1GzJ8eIk135x",
    "outputId": "625dac82-888f-4f43-d702-127787922466"
   },
   "outputs": [],
   "source": [
    "# Load audio data\n",
    "audio_data, audio_sr = librosa.load(input_path)\n",
    "\n",
    "# Audio\n",
    "ipd.Audio(audio_data, rate=audio_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:32.073891Z",
     "iopub.status.busy": "2024-11-03T10:56:32.072884Z",
     "iopub.status.idle": "2024-11-03T10:56:32.760803Z",
     "shell.execute_reply": "2024-11-03T10:56:32.759533Z",
     "shell.execute_reply.started": "2024-11-03T10:56:32.073839Z"
    },
    "executionInfo": {
     "elapsed": 1298,
     "status": "ok",
     "timestamp": 1708171713697,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "fxFZTmCE2Kdi",
    "outputId": "ace48685-fd2f-4191-bd75-0f8d701edfe0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "librosa.display.waveshow(y=audio_data, sr=audio_sr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:32.930344Z",
     "iopub.status.busy": "2024-11-03T10:56:32.929861Z",
     "iopub.status.idle": "2024-11-03T10:56:33.267965Z",
     "shell.execute_reply": "2024-11-03T10:56:33.266689Z",
     "shell.execute_reply.started": "2024-11-03T10:56:32.930298Z"
    },
    "executionInfo": {
     "elapsed": 72,
     "status": "ok",
     "timestamp": 1708171713715,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "s37raMyL2ip3",
    "outputId": "bba68be2-b02c-4d17-d76c-e1abc89aee22"
   },
   "outputs": [],
   "source": [
    "n_fft = 2048\n",
    "D = np.abs(librosa.stft(audio_data[:n_fft], n_fft=n_fft, hop_length=n_fft+1))\n",
    "plt.title('Spectrum');\n",
    "plt.xlabel('Frequency Bin');\n",
    "plt.ylabel('Amplitude');\n",
    "plt.plot(D);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:33.27138Z",
     "iopub.status.busy": "2024-11-03T10:56:33.27041Z",
     "iopub.status.idle": "2024-11-03T10:56:33.987996Z",
     "shell.execute_reply": "2024-11-03T10:56:33.98679Z",
     "shell.execute_reply.started": "2024-11-03T10:56:33.271322Z"
    },
    "executionInfo": {
     "elapsed": 2411,
     "status": "ok",
     "timestamp": 1708171716064,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "nPZkVqZd2mgy",
    "outputId": "73e5294b-2e42-42dd-9dd1-6b23c7305af5"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "spectrogram = librosa.feature.melspectrogram(y=audio_data, sr=audio_sr, n_mels=128,fmax=8000)\n",
    "log_spectrogram = librosa.power_to_db(spectrogram)\n",
    "librosa.display.specshow(log_spectrogram, y_axis='mel', sr=audio_sr, x_axis='time');\n",
    "plt.title('Mel Spectrogram ')\n",
    "plt.colorbar(format='%+2.0f dB');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "owS2hpJv7Lho"
   },
   "source": [
    "# 2. Speech Emotion Recognition Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:34.113371Z",
     "iopub.status.busy": "2024-11-03T10:56:34.112902Z",
     "iopub.status.idle": "2024-11-03T10:56:34.124478Z",
     "shell.execute_reply": "2024-11-03T10:56:34.123198Z",
     "shell.execute_reply.started": "2024-11-03T10:56:34.113327Z"
    },
    "executionInfo": {
     "elapsed": 517,
     "status": "ok",
     "timestamp": 1712643143257,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "q1f_S4f06-Zs"
   },
   "outputs": [],
   "source": [
    "def zcr(data,frame_length,hop_length):\n",
    "    zcr=librosa.feature.zero_crossing_rate(data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(zcr)\n",
    "\n",
    "def rmse(data,frame_length=2048,hop_length=512):\n",
    "    rmse=librosa.feature.rms(y=data,frame_length=frame_length,hop_length=hop_length)\n",
    "    return np.squeeze(rmse)\n",
    "\n",
    "def mfcc(data, sr, frame_length=2048, hop_length=512, flatten=True):\n",
    "    mfcc_result = librosa.feature.mfcc(y=data, sr=sr, n_fft=frame_length, hop_length=hop_length)\n",
    "    return np.squeeze(mfcc_result.T) if not flatten else np.ravel(mfcc_result.T)\n",
    "\n",
    "def extract_features(data,sr=22050,frame_length=2048,hop_length=512):\n",
    "    result=np.array([])\n",
    "\n",
    "    result=np.hstack((\n",
    "      result,\n",
    "      zcr(data,frame_length,hop_length),\n",
    "      rmse(data,frame_length,hop_length),\n",
    "      mfcc(data,sr,frame_length,hop_length)\n",
    "    ))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:34.75622Z",
     "iopub.status.busy": "2024-11-03T10:56:34.755779Z",
     "iopub.status.idle": "2024-11-03T10:56:34.765261Z",
     "shell.execute_reply": "2024-11-03T10:56:34.763933Z",
     "shell.execute_reply.started": "2024-11-03T10:56:34.756178Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712643143866,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "AN7uXkMl7X2F"
   },
   "outputs": [],
   "source": [
    "def get_predict_feat(path, expected_shape=(1, 2376)):\n",
    "    d, s_rate = librosa.load(path, duration=2.5, offset=0.6)\n",
    "    res = extract_features(d)\n",
    "\n",
    "    # Ensure res is reshaped or padded to match the expected shape\n",
    "    if res.shape != expected_shape:\n",
    "        flat_size = np.prod(expected_shape)\n",
    "        if res.size < flat_size:\n",
    "            # Pad if the size is smaller than expected\n",
    "            pad_width = (0, flat_size - res.size)\n",
    "            res = np.pad(res, pad_width=pad_width, mode='constant')\n",
    "        else:\n",
    "            # Resize if the size is larger than expected\n",
    "            res = np.resize(res, expected_shape)\n",
    "\n",
    "    i_result = scaler.transform(res.reshape(1, -1))\n",
    "    final_result = np.expand_dims(i_result, axis=2)\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:35.374426Z",
     "iopub.status.busy": "2024-11-03T10:56:35.373959Z",
     "iopub.status.idle": "2024-11-03T10:56:35.384638Z",
     "shell.execute_reply": "2024-11-03T10:56:35.383186Z",
     "shell.execute_reply.started": "2024-11-03T10:56:35.374382Z"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1712643145864,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "FYKmwrmE7aaf"
   },
   "outputs": [],
   "source": [
    "def prediction(path1):\n",
    "    res = get_predict_feat(path1)\n",
    "    predictions = loaded_model.predict(res)\n",
    "\n",
    "    # Get the label names or define them if available\n",
    "    label_names = list(encoder.categories_[0])\n",
    "\n",
    "    # Get the index of the label with the highest confidence score\n",
    "    predicted_label_index = np.argmax(predictions)\n",
    "\n",
    "    # List to store confidence scores\n",
    "    confidence_scores = []\n",
    "\n",
    "    # Display predicted emotion and confidence for each label\n",
    "    print(f\"\\nPredicted Emotion: {label_names[predicted_label_index]}\")\n",
    "    for label_index, label_name in enumerate(label_names):\n",
    "        confidence_score = predictions[0][label_index]\n",
    "        confidence_score = 0 if confidence_score < 0.001 else confidence_score\n",
    "        confidence_scores.append({'label': label_name, 'confidence': confidence_score})\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "    sorted_confidence_scores = sorted(confidence_scores, key=lambda x: x['confidence'], reverse=True)\n",
    "\n",
    "    return sorted_confidence_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-03T10:56:36.663387Z",
     "iopub.status.busy": "2024-11-03T10:56:36.662909Z",
     "iopub.status.idle": "2024-11-03T10:56:36.965826Z",
     "shell.execute_reply": "2024-11-03T10:56:36.964647Z",
     "shell.execute_reply.started": "2024-11-03T10:56:36.663344Z"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1708172075062,
     "user": {
      "displayName": "Rikin Zala",
      "userId": "12097819518315247097"
     },
     "user_tz": -330
    },
    "id": "9nudGrz37dMt",
    "outputId": "5f7405ca-f55d-4af3-e023-6f73f3ae1eb2"
   },
   "outputs": [],
   "source": [
    "prediction(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps \n",
    "\n",
    "## 1. Build A CNN Model for Text Emotion Recognition\n",
    "\n",
    "This notebook [Text Emotion Analysis using CNN](https://www.kaggle.com/code/rikinzala/text-emotion-analysis-using-cnn) will focus on creating NLP model that will analyse the emotions of text dataset. It is build upon the preprocessed and augmented text data to assess the model's performance on new data.\n",
    "\n",
    "## 2. Wrapping Up\n",
    "\n",
    "Since we are now with both Audio and Text ML models, we can create a python backend for Speech Emotion Recognition. Here is a github link for full implementation [SoulSyncopia](https://github.com/RikinZala25/Emotion-Music-Player). Consider checking all the branches.\n",
    "\n",
    "## If you find the helpful consider giving an upvote, Thanks!"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP5QUzCl8QKYritmdYs3Szv",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6000814,
     "sourceId": 9793951,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 204955596,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python (mlenv)",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
